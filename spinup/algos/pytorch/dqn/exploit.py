import numpy as np
import torch
import gym
import spinup.algos.pytorch.dqn.core as core
from spinup.utils.logx import EpochLogger
import os.path as osp
import joblib


def exploit(env_fn, fpath="", epsilon=0.0, start_xi=np.inf,
            xi_update_fn=core.id_update, start_xi_fn=core.use_start_xi,
            seed=0, num_test_episodes=100, max_ep_len=None, gamma=0.99,
            epsilon_greedy=False,
            logger_kwargs=dict(), save_freq=100):
    """
    Measure the quality and number of actions needed by different policies given a Q-value function.
    """
    logger = EpochLogger(**logger_kwargs)
    logger.save_config(locals())

    if isinstance(xi_update_fn, str):
        xi_update_fn = eval(xi_update_fn)

    if isinstance(start_xi_fn, str):
        start_xi_fn = eval(start_xi_fn)

    itr = ''
    fname = osp.join(fpath, 'pyt_save', 'model' + itr + '.pt')
    print('\n\nLoading from %s.\n\n' % fname)
    q = torch.load(fname)

    try:
        state = joblib.load(osp.join(fpath, 'vars'+itr+'.pkl'))
        env = state['env']
    except:
        env = None

    torch.manual_seed(seed)
    np.random.seed(seed)

    logger.log(f'\nNumber of parameters: \t {core.count_vars(q)}')

    def get_action(o, epsilon_greedy=True, epsilon=0.0, xi=np.inf):
        return q.satisfice(torch.as_tensor(o, dtype=torch.float32), epsilon_greedy=epsilon_greedy, epsilon=epsilon, xi=xi)

    def test_agent():
        o, d, ep_ret, ep_len, ep_acts = env.reset(), False, 0, 0, 0
        _, _, q_values = get_action(o, epsilon_greedy=epsilon_greedy, epsilon=epsilon, xi=np.inf)
        if not epsilon_greedy:
            xi = start_xi_fn(start_xi=start_xi, q_values=q_values.numpy())
        else:
            xi = np.inf
        logger.store(StartingXi=xi)
        while not(d or (ep_len == max_ep_len)):
            # Take greedy actions at test time
            action, num_actions, q_values = get_action(o, epsilon_greedy=epsilon_greedy, epsilon=epsilon, xi=xi)
            o, r, d, _ = env.step(action)
            if not epsilon_greedy:
                xi = xi_update_fn(xi=xi, reward=r, t=ep_len, gamma=gamma, q_value=q_values[action].numpy())
            ep_ret += r
            ep_len += 1
            ep_acts += num_actions
        logger.store(TestEpRet=ep_ret, TestEpLen=ep_len, TestEpActions=ep_acts)

    # Main loop: collect experience in env and update/log each epoch
    for t in range(num_test_episodes):
        test_agent()
        if (t+1) % save_freq == 0:
            # Log info about epoch
            logger.log_tabular('TestEpRet')
            logger.log_tabular('TestEpActions')
            logger.log_tabular('StartingXi', with_min_and_max=True)
            logger.log_tabular('TestEpLen', average_only=True)
            logger.log_tabular('TotalEnvInteracts', t)
            logger.dump_tabular()
